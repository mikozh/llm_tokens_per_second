# llm_tokens_per_second
Simple script to estimate LLM throughput of a served model 
